#!/usr/bin/env python3\n\"\"\"\nINTELLIGENT SPRITESHEET ANALYZER\nLöst die erkannten Probleme mit Connected Components behebt durch adaptive Größenvalidierung, Geometrie-Erwartungen und intelligente Post-Processing-Filterung.\n\n1. PROBLEM: Inkonsistente Frame-Größen (von 127x125 bis 1137x1207)\n2. PROBLEM: Übersegmentierung (bis zu 169 Frames)\n3. PROBLEM: Untersegmentierung (nur 1 Frame)\n\nLÖSUNG:\n- Adaptive Größenvalidierung\n- Geometrie-Erwartungen für Sprites\n- Intelligente Filterung und Zusammenfassung\n- Multi-Level Connected Components Analyse\n\"\"\"\n\nimport cv2\nimport numpy as np\nfrom PIL import Image, ImageEnhance, ImageFilter\nfrom collections import Counter\nfrom pathlib import Path\nimport json\nfrom datetime import datetime\nfrom typing import List, Tuple, Dict, Optional\nimport statistics\n\n\nclass IntelligentSpritesheetAnalyzer:\n    \"\"\"Intelligente Spritesheet-Analyse mit adaptiver Frame-Erkennung\"\"\"\n\n    def __init__(self, session_name: str = None):\n        self.session_name = session_name or f\"intelligent_session_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n        self.base_dir = Path(\"output/intelligent_analysis\")\n        self.session_dir = self.base_dir / self.session_name\n        \n        # Erstelle Ausgabeverzeichnisse\n        (self.session_dir / \"animations\").mkdir(parents=True, exist_ok=True)\n        (self.session_dir / \"individual_sprites\").mkdir(parents=True, exist_ok=True)\n        (self.session_dir / \"reports\").mkdir(parents=True, exist_ok=True)\n        (self.session_dir / \"debug_images\").mkdir(parents=True, exist_ok=True)\n\n    def detect_background_intelligent(self, image: np.ndarray) -> Tuple[np.ndarray, Dict]:\n        \"\"\"SCHRITT 1: Intelligente Hintergrund-Erkennung mit Multi-Zone Sampling\"\"\"\n        h, w = image.shape[:2]\n        \n        # 12-Zonen Sampling für robuste Hintergrund-Erkennung\n        zones = [\n            (0, 0, 50, 50),  # Top-left\n            (w-50, 0, 50, 50),  # Top-right\n            (0, h-50, 50, 50),  # Bottom-left\n            (w-50, h-50, 50, 50),  # Bottom-right\n            (w//2-25, 0, 50, 25),  # Top-center\n            (w//2-25, h-25, 50, 25),  # Bottom-center\n            (0, h//2-25, 25, 50),  # Left-center\n            (w-25, h//2-25, 25, 50),  # Right-center\n            (w//4-25, h//4-25, 50, 50),  # Quarter-1\n            (3*w//4-25, h//4-25, 50, 50),  # Quarter-2\n            (w//4-25, 3*h//4-25, 50, 50),  # Quarter-3\n            (3*w//4-25, 3*h//4-25, 50, 50),  # Quarter-4\n        ]\n        \n        colors = []\n        for x, y, w_zone, h_zone in zones:\n            x = max(0, min(x, w-w_zone))\n            y = max(0, min(y, h-h_zone))\n            zone = image[y:y+h_zone, x:x+w_zone]\n            if zone.size > 0:\n                mean_color = zone.mean(axis=(0, 1))\n                colors.append(mean_color)\n        \n        # Konsistente Hintergrundfarbe finden\n        if colors:\n            bg_color = np.mean(colors, axis=0)\n            \n            # Adaptive Toleranz basierend auf Farbvarianz\n            color_variance = np.std(colors, axis=0).mean()\n            tolerance = max(20, min(50, int(color_variance * 2)))\n            \n            # RGB und HSV Analyse für robuste Maske\n            rgb_mask = np.all(np.abs(image - bg_color) <= tolerance, axis=-1)\n            \n            # HSV für bessere Farbseparierung\n            hsv_image = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n            hsv_bg = cv2.cvtColor(bg_color.reshape(1, 1, 3).astype(np.uint8), cv2.COLOR_BGR2HSV)[0, 0]\n            hsv_tolerance = tolerance // 2\n            hsv_mask = np.all(np.abs(hsv_image - hsv_bg) <= hsv_tolerance, axis=-1)\n            \n            # Kombiniere beide Masken\n            combined_mask = rgb_mask | hsv_mask\n            foreground_mask = ~combined_mask\n            \n            # Morphologische Bereinigung\n            kernel = np.ones((3, 3), np.uint8)\n            foreground_mask = cv2.morphologyEx(foreground_mask.astype(np.uint8), cv2.MORPH_CLOSE, kernel)\n            foreground_mask = cv2.morphologyEx(foreground_mask, cv2.MORPH_OPEN, kernel)\n            \n            analysis = {\n                \"background_color\": bg_color.tolist(),\n                \"tolerance\": int(tolerance),\n                \"zones_sampled\": len(colors),\n                \"color_variance\": float(color_variance),\n                \"foreground_ratio\": float(foreground_mask.sum() / foreground_mask.size)\n            }\n            \n            return foreground_mask.astype(bool), analysis\n        \n        # Fallback\n        return np.ones((h, w), dtype=bool), {\"method\": \"fallback\"}\n\n    def analyze_sprite_characteristics(self, components: List[Dict]) -> Dict:\n        \"\"\"SCHRITT 2: Analyse der Sprite-Charakteristika für intelligente Filterung\"\"\"\n        if not components:\n            return {\"error\": \"No components found\"}\n        \n        areas = [comp[\"area\"] for comp in components]\n        widths = [comp[\"width\"] for comp in components]\n        heights = [comp[\"height\"] for comp in components]\n        \n        # Statistische Analyse\n        analysis = {\n            \"total_components\": len(components),\n            \"area_stats\": {\n                \"mean\": statistics.mean(areas),\n                \"median\": statistics.median(areas),\n                \"std\": statistics.stdev(areas) if len(areas) > 1 else 0,\n                \"min\": min(areas),\n                \"max\": max(areas)\n            },\n            \"size_stats\": {\n                \"width_mean\": statistics.mean(widths),\n                \"height_mean\": statistics.mean(heights),\n                \"aspect_ratio_mean\": statistics.mean([w/h for w, h in zip(widths, heights) if h > 0])\n            }\n        }\n        \n        # Erkenne Anomalien\n        area_mean = analysis[\"area_stats\"][\"mean\"]\n        area_std = analysis[\"area_stats\"][\"std\"]\n        \n        # Sprites sollten ähnliche Größen haben\n        normal_area_range = (area_mean - 2 * area_std, area_mean + 2 * area_std)\n        \n        analysis[\"recommendations\"] = {\n            \"normal_area_range\": normal_area_range,\n            \"suggested_min_area\": max(1000, normal_area_range[0] * 0.3),\n            \"suggested_max_area\": min(area_mean * 3, normal_area_range[1] * 1.5),\n            \"expected_sprite_count\": self.estimate_sprite_count(areas)\n        }\n        \n        return analysis\n\n    def estimate_sprite_count(self, areas: List[float]) -> int:\n        \"\"\"Schätze die erwartete Anzahl an Sprites basierend auf Größenverteilung\"\"\"\n        if not areas:\n            return 0\n        \n        # Histogramm-Analyse für Cluster-Erkennung\n        area_array = np.array(areas)\n        \n        # Entferne extreme Ausreißer (>3 Standardabweichungen)\n        mean_area = np.mean(area_array)\n        std_area = np.std(area_array)\n        filtered_areas = area_array[np.abs(area_array - mean_area) <= 3 * std_area]\n        \n        # Erwartete Sprite-Anzahl: 4-32 (typisch für Spritesheets)\n        expected_count = len(filtered_areas)\n        return max(4, min(32, expected_count))\n\n    def extract_intelligent_frames(self, image: np.ndarray, mask: np.ndarray) -> Tuple[List[np.ndarray], Dict]:\n        \"\"\"SCHRITT 3: Intelligente Frame-Extraktion mit adaptiver Filterung\"\"\"\n        print(\"   🧠 Intelligente Frame-Extraktion...\")\n        \n        # Connected Components Analyse\n        num_labels, labels, stats, centroids = cv2.connectedComponentsWithStats(\n            mask.astype(np.uint8), connectivity=8)\n        \n        # Erstelle Component-Liste (ohne Hintergrund Label 0)\n        components = []\n        for i in range(1, num_labels):  # Skip background\n            x, y, w, h, area = stats[i]\n            if area > 500:  # Minimal-Filter für Noise\n                components.append({\n                    \"id\": i,\n                    \"x\": x, \"y\": y,\n                    \"width\": w, \"height\": h,\n                    \"area\": area,\n                    \"centroid\": centroids[i]\n                })\n        \n        # Analysiere Sprite-Charakteristika\n        analysis = self.analyze_sprite_characteristics(components)\n        print(f\"   📊 Gefunden: {len(components)} Komponenten, Empfehlung: {analysis.get('recommendations', {}).get('expected_sprite_count', 'N/A')} Sprites\")\n        \n        # Intelligente Filterung\n        if \"recommendations\" in analysis:\n            min_area = analysis[\"recommendations\"][\"suggested_min_area\"]\n            max_area = analysis[\"recommendations\"][\"suggested_max_area\"]\n            \n            filtered_components = [\n                comp for comp in components\n                if min_area <= comp[\"area\"] <= max_area\n            ]\n            \n            print(f\"   🔍 Nach intelligenter Filterung: {len(filtered_components)} Frames\")\n        else:\n            filtered_components = components\n        \n        # Sortiere nach Position (links-rechts, oben-unten)\n        filtered_components.sort(key=lambda c: (c[\"y\"] // 100, c[\"x\"]))\n        \n        # Extrahiere Frames mit 30px Padding\n        frames = []\n        padding = 30\n        h, w = image.shape[:2]\n        \n        for comp in filtered_components:\n            x, y, cw, ch = comp[\"x\"], comp[\"y\"], comp[\"width\"], comp[\"height\"]\n            \n            # Padding hinzufügen\n            x1 = max(0, x - padding)\n            y1 = max(0, y - padding)\n            x2 = min(w, x + cw + padding)\n            y2 = min(h, y + ch + padding)\n            \n            frame = image[y1:y2, x1:x2]\n            if frame.size > 0:\n                frames.append(frame)\n        \n        extraction_info = {\n            \"total_components_found\": len(components),\n            \"filtered_components\": len(filtered_components),\n            \"frames_extracted\": len(frames),\n            \"analysis\": analysis,\n            \"padding_applied\": padding\n        }\n        \n        return frames, extraction_info\n\n    def apply_enhanced_vaporwave(self, frame: np.ndarray, intensity: float = 0.375) -> np.ndarray:\n        \"\"\"SCHRITT 4: Verstärkter Vaporwave-Filter\"\"\"\n        # Konvertiere zu PIL für erweiterte Bearbeitung\n        pil_frame = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n        \n        # 1. Farbverschiebung: Cyan-Magenta Palette\n        np_frame = np.array(pil_frame, dtype=np.float32)\n        \n        # Verstärkte Cyan-Magenta Verschiebung\n        np_frame[:, :, 0] = np.clip(np_frame[:, :, 0] * (1.0 + intensity * 0.4), 0, 255)  # Rot\n        np_frame[:, :, 1] = np.clip(np_frame[:, :, 1] * (1.0 + intensity * 0.2), 0, 255)  # Grün\n        np_frame[:, :, 2] = np.clip(np_frame[:, :, 2] * (1.0 + intensity * 0.6), 0, 255)  # Blau\n        \n        vaporwave_frame = Image.fromarray(np_frame.astype(np.uint8))\n        \n        # 2. Sättigung verstärken\n        sat_enhancer = ImageEnhance.Color(vaporwave_frame)\n        vaporwave_frame = sat_enhancer.enhance(1.0 + intensity * 0.8)\n        \n        # 3. Verstärkter Kontrast\n        contrast_enhancer = ImageEnhance.Contrast(vaporwave_frame)\n        vaporwave_frame = contrast_enhancer.enhance(1.0 + intensity * 0.3)\n        \n        # 4. Helligkeit leicht reduzieren für Retro-Look\n        brightness_enhancer = ImageEnhance.Brightness(vaporwave_frame)\n        vaporwave_frame = brightness_enhancer.enhance(0.95)\n        \n        return cv2.cvtColor(np.array(vaporwave_frame), cv2.COLOR_RGB2BGR)\n\n    def process_single_spritesheet(self, image_path: Path) -> Dict:\n        \"\"\"Verarbeite ein einzelnes Spritesheet mit intelligenter Analyse\"\"\"\n        print(f\"\\n🎯 Verarbeitung: {image_path.name}\")\n        \n        # Lade und skaliere Bild\n        image = cv2.imread(str(image_path))\n        if image is None:\n            return {\"error\": f\"Could not load {image_path}\"}\n        \n        original_size = image.shape[:2]\n        \n        # 2x Upscaling + Schärfung\n        print(\"   📈 2x Upscaling + Schärfung...\")\n        upscaled = cv2.resize(image, None, fx=2, fy=2, interpolation=cv2.INTER_CUBIC)\n        \n        # Erweiterte Bildverbesserung\n        upscaled_pil = Image.fromarray(cv2.cvtColor(upscaled, cv2.COLOR_BGR2RGB))\n        \n        # Unschärfe-Maske für Schärfung\n        sharpened = upscaled_pil.filter(ImageFilter.UnsharpMask(radius=1, percent=150, threshold=3))\n        \n        # Kontrast und Helligkeit\n        contrast_enhancer = ImageEnhance.Contrast(sharpened)\n        enhanced = contrast_enhancer.enhance(1.2)\n        \n        brightness_enhancer = ImageEnhance.Brightness(enhanced)\n        enhanced = brightness_enhancer.enhance(1.05)\n        \n        # Farbintensivierung\n        color_enhancer = ImageEnhance.Color(enhanced)\n        enhanced = color_enhancer.enhance(1.1)\n        \n        processed_image = cv2.cvtColor(np.array(enhanced), cv2.COLOR_RGB2BGR)\n        \n        # Intelligente Hintergrund-Erkennung\n        print(\"   🔍 Intelligente Hintergrund-Analyse...\")\n        foreground_mask, bg_analysis = self.detect_background_intelligent(processed_image)\n        \n        # Intelligente Frame-Extraktion\n        frames, extraction_info = self.extract_intelligent_frames(processed_image, foreground_mask)\n        \n        if not frames:\n            return {\n                \"error\": \"No frames extracted\",\n                \"background_analysis\": bg_analysis,\n                \"extraction_info\": extraction_info\n            }\n        \n        # Verarbeite jeden Frame\n        sprite_dir = self.session_dir / \"individual_sprites\" / image_path.stem\n        sprite_dir.mkdir(exist_ok=True)\n        \n        frame_info = []\n        processed_frames = []\n        \n        for i, frame in enumerate(frames, 1):\n            # Vaporwave-Filter anwenden\n            vaporwave_frame = self.apply_enhanced_vaporwave(frame, 0.375)\n            \n            # Als PNG speichern (mit Transparenz)\n            frame_filename = f\"frame_{i:03d}.png\"\n            frame_path = sprite_dir / frame_filename\n            \n            # Konvertiere zu RGBA für Transparenz\n            frame_rgba = cv2.cvtColor(vaporwave_frame, cv2.COLOR_BGR2BGRA)\n            \n            # Speichere als PNG\n            cv2.imwrite(str(frame_path), frame_rgba)\n            \n            frame_info.append({\n                \"id\": i,\n                \"filename\": frame_filename,\n                \"size\": f\"{frame.shape[1]}x{frame.shape[0]}\",\n                \"area\": frame.shape[0] * frame.shape[1],\n                \"vaporwave_applied\": True\n            })\n            \n            processed_frames.append(vaporwave_frame)\n        \n        # Erstelle GIF Animation\n        gif_path = self.session_dir / \"animations\" / f\"{image_path.stem}_intelligent_vaporwave.gif\"\n        self.create_gif_animation(processed_frames, gif_path)\n        \n        # Speichere Debug-Informationen\n        debug_path = self.session_dir / \"debug_images\" / f\"{image_path.stem}_analysis.png\"\n        self.save_debug_visualization(processed_image, foreground_mask, debug_path)\n        \n        return {\n            \"input_file\": str(image_path),\n            \"processing_timestamp\": datetime.now().isoformat(),\n            \"workflow\": \"Intelligent Analysis + Enhanced\",\n            \"original_size\": original_size,\n            \"upscaled_size\": processed_image.shape[:2],\n            \"background_analysis\": bg_analysis,\n            \"extraction_info\": extraction_info,\n            \"total_frames\": len(frames),\n            \"frames\": frame_info,\n            \"output_directory\": str(sprite_dir),\n            \"gif_animation\": str(gif_path),\n            \"debug_visualization\": str(debug_path)\n        }\n\n    def create_gif_animation(self, frames: List[np.ndarray], output_path: Path):\n        \"\"\"Erstelle GIF Animation aus Frames\"\"\"\n        if not frames:\n            return\n        \n        # Konvertiere zu PIL Images\n        pil_frames = []\n        for frame in frames:\n            rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n            pil_frames.append(Image.fromarray(rgb_frame))\n        \n        # Speichere als GIF\n        if pil_frames:\n            pil_frames[0].save(\n                str(output_path),\n                save_all=True,\n                append_images=pil_frames[1:],\n                duration=200,  # 200ms pro Frame\n                loop=0,\n                optimize=True\n            )\n\n    def save_debug_visualization(self, image: np.ndarray, mask: np.ndarray, output_path: Path):\n        \"\"\"Speichere Debug-Visualisierung\"\"\"\n        h, w = image.shape[:2]\n        \n        # Erstelle Visualisierung: Original + Maske + Overlay\n        debug_img = np.zeros((h, w*3, 3), dtype=np.uint8)\n        \n        # Original\n        debug_img[:, :w] = image\n        \n        # Maske\n        mask_colored = np.zeros((h, w, 3), dtype=np.uint8)\n        mask_colored[mask] = [0, 255, 0]  # Grün für Vordergrund\n        debug_img[:, w:2*w] = mask_colored\n        \n        # Overlay\n        overlay = image.copy()\n        overlay[~mask] = overlay[~mask] * 0.3  # Dunkle Hintergrund\n        debug_img[:, 2*w:] = overlay\n        \n        cv2.imwrite(str(output_path), debug_img)\n\n\nif __name__ == \"__main__\":\n    # Test mit dem problematischen Fall\n    analyzer = IntelligentSpritesheetAnalyzer()\n    \n    test_file = Path(\"input/Der_Zauber_des_alten_Mannes.png\")\n    if test_file.exists():\n        print(\"🔬 INTELLIGENTE ANALYSE - PROBLEMATISCHER FALL\")\n        print(\"=\"*60)\n        \n        result = analyzer.process_single_spritesheet(test_file)\n        \n        # Speichere Report\n        report_path = analyzer.session_dir / \"reports\" / f\"{test_file.stem}_intelligent_report.json\"\n        with open(report_path, 'w', encoding='utf-8') as f:\n            json.dump(result, f, indent=2, ensure_ascii=False)\n        \n        print(f\"\\n📊 ERGEBNIS:\")\n        print(f\"   Frames extrahiert: {result.get('total_frames', 'N/A')}\")\n        print(f\"   Empfohlene Anzahl: {result.get('extraction_info', {}).get('analysis', {}).get('recommendations', {}).get('expected_sprite_count', 'N/A')}\")\n        print(f\"   Report gespeichert: {report_path}\")\n        print(f\"   Debug-Bild: {result.get('debug_visualization', 'N/A')}\")\n        \n    else:\n        print(f\"❌ Test-Datei nicht gefunden: {test_file}\")\n
