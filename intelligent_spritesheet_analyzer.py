#!/usr/bin/env python3\n\"\"\"\nINTELLIGENT SPRITESHEET ANALYZER\nL√∂st die erkannten Probleme mit Connected Components behebt durch adaptive Gr√∂√üenvalidierung, Geometrie-Erwartungen und intelligente Post-Processing-Filterung.\n\n1. PROBLEM: Inkonsistente Frame-Gr√∂√üen (von 127x125 bis 1137x1207)\n2. PROBLEM: √úbersegmentierung (bis zu 169 Frames)\n3. PROBLEM: Untersegmentierung (nur 1 Frame)\n\nL√ñSUNG:\n- Adaptive Gr√∂√üenvalidierung\n- Geometrie-Erwartungen f√ºr Sprites\n- Intelligente Filterung und Zusammenfassung\n- Multi-Level Connected Components Analyse\n\"\"\"\n\nimport cv2\nimport numpy as np\nfrom PIL import Image, ImageEnhance, ImageFilter\nfrom collections import Counter\nfrom pathlib import Path\nimport json\nfrom datetime import datetime\nfrom typing import List, Tuple, Dict, Optional\nimport statistics\n\n\nclass IntelligentSpritesheetAnalyzer:\n    \"\"\"Intelligente Spritesheet-Analyse mit adaptiver Frame-Erkennung\"\"\"\n\n    def __init__(self, session_name: str = None):\n        self.session_name = session_name or f\"intelligent_session_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n        self.base_dir = Path(\"output/intelligent_analysis\")\n        self.session_dir = self.base_dir / self.session_name\n        \n        # Erstelle Ausgabeverzeichnisse\n        (self.session_dir / \"animations\").mkdir(parents=True, exist_ok=True)\n        (self.session_dir / \"individual_sprites\").mkdir(parents=True, exist_ok=True)\n        (self.session_dir / \"reports\").mkdir(parents=True, exist_ok=True)\n        (self.session_dir / \"debug_images\").mkdir(parents=True, exist_ok=True)\n\n    def detect_background_intelligent(self, image: np.ndarray) -> Tuple[np.ndarray, Dict]:\n        \"\"\"SCHRITT 1: Intelligente Hintergrund-Erkennung mit Multi-Zone Sampling\"\"\"\n        h, w = image.shape[:2]\n        \n        # 12-Zonen Sampling f√ºr robuste Hintergrund-Erkennung\n        zones = [\n            (0, 0, 50, 50),  # Top-left\n            (w-50, 0, 50, 50),  # Top-right\n            (0, h-50, 50, 50),  # Bottom-left\n            (w-50, h-50, 50, 50),  # Bottom-right\n            (w//2-25, 0, 50, 25),  # Top-center\n            (w//2-25, h-25, 50, 25),  # Bottom-center\n            (0, h//2-25, 25, 50),  # Left-center\n            (w-25, h//2-25, 25, 50),  # Right-center\n            (w//4-25, h//4-25, 50, 50),  # Quarter-1\n            (3*w//4-25, h//4-25, 50, 50),  # Quarter-2\n            (w//4-25, 3*h//4-25, 50, 50),  # Quarter-3\n            (3*w//4-25, 3*h//4-25, 50, 50),  # Quarter-4\n        ]\n        \n        colors = []\n        for x, y, w_zone, h_zone in zones:\n            x = max(0, min(x, w-w_zone))\n            y = max(0, min(y, h-h_zone))\n            zone = image[y:y+h_zone, x:x+w_zone]\n            if zone.size > 0:\n                mean_color = zone.mean(axis=(0, 1))\n                colors.append(mean_color)\n        \n        # Konsistente Hintergrundfarbe finden\n        if colors:\n            bg_color = np.mean(colors, axis=0)\n            \n            # Adaptive Toleranz basierend auf Farbvarianz\n            color_variance = np.std(colors, axis=0).mean()\n            tolerance = max(20, min(50, int(color_variance * 2)))\n            \n            # RGB und HSV Analyse f√ºr robuste Maske\n            rgb_mask = np.all(np.abs(image - bg_color) <= tolerance, axis=-1)\n            \n            # HSV f√ºr bessere Farbseparierung\n            hsv_image = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n            hsv_bg = cv2.cvtColor(bg_color.reshape(1, 1, 3).astype(np.uint8), cv2.COLOR_BGR2HSV)[0, 0]\n            hsv_tolerance = tolerance // 2\n            hsv_mask = np.all(np.abs(hsv_image - hsv_bg) <= hsv_tolerance, axis=-1)\n            \n            # Kombiniere beide Masken\n            combined_mask = rgb_mask | hsv_mask\n            foreground_mask = ~combined_mask\n            \n            # Morphologische Bereinigung\n            kernel = np.ones((3, 3), np.uint8)\n            foreground_mask = cv2.morphologyEx(foreground_mask.astype(np.uint8), cv2.MORPH_CLOSE, kernel)\n            foreground_mask = cv2.morphologyEx(foreground_mask, cv2.MORPH_OPEN, kernel)\n            \n            analysis = {\n                \"background_color\": bg_color.tolist(),\n                \"tolerance\": int(tolerance),\n                \"zones_sampled\": len(colors),\n                \"color_variance\": float(color_variance),\n                \"foreground_ratio\": float(foreground_mask.sum() / foreground_mask.size)\n            }\n            \n            return foreground_mask.astype(bool), analysis\n        \n        # Fallback\n        return np.ones((h, w), dtype=bool), {\"method\": \"fallback\"}\n\n    def analyze_sprite_characteristics(self, components: List[Dict]) -> Dict:\n        \"\"\"SCHRITT 2: Analyse der Sprite-Charakteristika f√ºr intelligente Filterung\"\"\"\n        if not components:\n            return {\"error\": \"No components found\"}\n        \n        areas = [comp[\"area\"] for comp in components]\n        widths = [comp[\"width\"] for comp in components]\n        heights = [comp[\"height\"] for comp in components]\n        \n        # Statistische Analyse\n        analysis = {\n            \"total_components\": len(components),\n            \"area_stats\": {\n                \"mean\": statistics.mean(areas),\n                \"median\": statistics.median(areas),\n                \"std\": statistics.stdev(areas) if len(areas) > 1 else 0,\n                \"min\": min(areas),\n                \"max\": max(areas)\n            },\n            \"size_stats\": {\n                \"width_mean\": statistics.mean(widths),\n                \"height_mean\": statistics.mean(heights),\n                \"aspect_ratio_mean\": statistics.mean([w/h for w, h in zip(widths, heights) if h > 0])\n            }\n        }\n        \n        # Erkenne Anomalien\n        area_mean = analysis[\"area_stats\"][\"mean\"]\n        area_std = analysis[\"area_stats\"][\"std\"]\n        \n        # Sprites sollten √§hnliche Gr√∂√üen haben\n        normal_area_range = (area_mean - 2 * area_std, area_mean + 2 * area_std)\n        \n        analysis[\"recommendations\"] = {\n            \"normal_area_range\": normal_area_range,\n            \"suggested_min_area\": max(1000, normal_area_range[0] * 0.3),\n            \"suggested_max_area\": min(area_mean * 3, normal_area_range[1] * 1.5),\n            \"expected_sprite_count\": self.estimate_sprite_count(areas)\n        }\n        \n        return analysis\n\n    def estimate_sprite_count(self, areas: List[float]) -> int:\n        \"\"\"Sch√§tze die erwartete Anzahl an Sprites basierend auf Gr√∂√üenverteilung\"\"\"\n        if not areas:\n            return 0\n        \n        # Histogramm-Analyse f√ºr Cluster-Erkennung\n        area_array = np.array(areas)\n        \n        # Entferne extreme Ausrei√üer (>3 Standardabweichungen)\n        mean_area = np.mean(area_array)\n        std_area = np.std(area_array)\n        filtered_areas = area_array[np.abs(area_array - mean_area) <= 3 * std_area]\n        \n        # Erwartete Sprite-Anzahl: 4-32 (typisch f√ºr Spritesheets)\n        expected_count = len(filtered_areas)\n        return max(4, min(32, expected_count))\n\n    def extract_intelligent_frames(self, image: np.ndarray, mask: np.ndarray) -> Tuple[List[np.ndarray], Dict]:\n        \"\"\"SCHRITT 3: Intelligente Frame-Extraktion mit adaptiver Filterung\"\"\"\n        print(\"   üß† Intelligente Frame-Extraktion...\")\n        \n        # Connected Components Analyse\n        num_labels, labels, stats, centroids = cv2.connectedComponentsWithStats(\n            mask.astype(np.uint8), connectivity=8)\n        \n        # Erstelle Component-Liste (ohne Hintergrund Label 0)\n        components = []\n        for i in range(1, num_labels):  # Skip background\n            x, y, w, h, area = stats[i]\n            if area > 500:  # Minimal-Filter f√ºr Noise\n                components.append({\n                    \"id\": i,\n                    \"x\": x, \"y\": y,\n                    \"width\": w, \"height\": h,\n                    \"area\": area,\n                    \"centroid\": centroids[i]\n                })\n        \n        # Analysiere Sprite-Charakteristika\n        analysis = self.analyze_sprite_characteristics(components)\n        print(f\"   üìä Gefunden: {len(components)} Komponenten, Empfehlung: {analysis.get('recommendations', {}).get('expected_sprite_count', 'N/A')} Sprites\")\n        \n        # Intelligente Filterung\n        if \"recommendations\" in analysis:\n            min_area = analysis[\"recommendations\"][\"suggested_min_area\"]\n            max_area = analysis[\"recommendations\"][\"suggested_max_area\"]\n            \n            filtered_components = [\n                comp for comp in components\n                if min_area <= comp[\"area\"] <= max_area\n            ]\n            \n            print(f\"   üîç Nach intelligenter Filterung: {len(filtered_components)} Frames\")\n        else:\n            filtered_components = components\n        \n        # Sortiere nach Position (links-rechts, oben-unten)\n        filtered_components.sort(key=lambda c: (c[\"y\"] // 100, c[\"x\"]))\n        \n        # Extrahiere Frames mit 30px Padding\n        frames = []\n        padding = 30\n        h, w = image.shape[:2]\n        \n        for comp in filtered_components:\n            x, y, cw, ch = comp[\"x\"], comp[\"y\"], comp[\"width\"], comp[\"height\"]\n            \n            # Padding hinzuf√ºgen\n            x1 = max(0, x - padding)\n            y1 = max(0, y - padding)\n            x2 = min(w, x + cw + padding)\n            y2 = min(h, y + ch + padding)\n            \n            frame = image[y1:y2, x1:x2]\n            if frame.size > 0:\n                frames.append(frame)\n        \n        extraction_info = {\n            \"total_components_found\": len(components),\n            \"filtered_components\": len(filtered_components),\n            \"frames_extracted\": len(frames),\n            \"analysis\": analysis,\n            \"padding_applied\": padding\n        }\n        \n        return frames, extraction_info\n\n    def apply_enhanced_vaporwave(self, frame: np.ndarray, intensity: float = 0.375) -> np.ndarray:\n        \"\"\"SCHRITT 4: Verst√§rkter Vaporwave-Filter\"\"\"\n        # Konvertiere zu PIL f√ºr erweiterte Bearbeitung\n        pil_frame = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n        \n        # 1. Farbverschiebung: Cyan-Magenta Palette\n        np_frame = np.array(pil_frame, dtype=np.float32)\n        \n        # Verst√§rkte Cyan-Magenta Verschiebung\n        np_frame[:, :, 0] = np.clip(np_frame[:, :, 0] * (1.0 + intensity * 0.4), 0, 255)  # Rot\n        np_frame[:, :, 1] = np.clip(np_frame[:, :, 1] * (1.0 + intensity * 0.2), 0, 255)  # Gr√ºn\n        np_frame[:, :, 2] = np.clip(np_frame[:, :, 2] * (1.0 + intensity * 0.6), 0, 255)  # Blau\n        \n        vaporwave_frame = Image.fromarray(np_frame.astype(np.uint8))\n        \n        # 2. S√§ttigung verst√§rken\n        sat_enhancer = ImageEnhance.Color(vaporwave_frame)\n        vaporwave_frame = sat_enhancer.enhance(1.0 + intensity * 0.8)\n        \n        # 3. Verst√§rkter Kontrast\n        contrast_enhancer = ImageEnhance.Contrast(vaporwave_frame)\n        vaporwave_frame = contrast_enhancer.enhance(1.0 + intensity * 0.3)\n        \n        # 4. Helligkeit leicht reduzieren f√ºr Retro-Look\n        brightness_enhancer = ImageEnhance.Brightness(vaporwave_frame)\n        vaporwave_frame = brightness_enhancer.enhance(0.95)\n        \n        return cv2.cvtColor(np.array(vaporwave_frame), cv2.COLOR_RGB2BGR)\n\n    def process_single_spritesheet(self, image_path: Path) -> Dict:\n        \"\"\"Verarbeite ein einzelnes Spritesheet mit intelligenter Analyse\"\"\"\n        print(f\"\\nüéØ Verarbeitung: {image_path.name}\")\n        \n        # Lade und skaliere Bild\n        image = cv2.imread(str(image_path))\n        if image is None:\n            return {\"error\": f\"Could not load {image_path}\"}\n        \n        original_size = image.shape[:2]\n        \n        # 2x Upscaling + Sch√§rfung\n        print(\"   üìà 2x Upscaling + Sch√§rfung...\")\n        upscaled = cv2.resize(image, None, fx=2, fy=2, interpolation=cv2.INTER_CUBIC)\n        \n        # Erweiterte Bildverbesserung\n        upscaled_pil = Image.fromarray(cv2.cvtColor(upscaled, cv2.COLOR_BGR2RGB))\n        \n        # Unsch√§rfe-Maske f√ºr Sch√§rfung\n        sharpened = upscaled_pil.filter(ImageFilter.UnsharpMask(radius=1, percent=150, threshold=3))\n        \n        # Kontrast und Helligkeit\n        contrast_enhancer = ImageEnhance.Contrast(sharpened)\n        enhanced = contrast_enhancer.enhance(1.2)\n        \n        brightness_enhancer = ImageEnhance.Brightness(enhanced)\n        enhanced = brightness_enhancer.enhance(1.05)\n        \n        # Farbintensivierung\n        color_enhancer = ImageEnhance.Color(enhanced)\n        enhanced = color_enhancer.enhance(1.1)\n        \n        processed_image = cv2.cvtColor(np.array(enhanced), cv2.COLOR_RGB2BGR)\n        \n        # Intelligente Hintergrund-Erkennung\n        print(\"   üîç Intelligente Hintergrund-Analyse...\")\n        foreground_mask, bg_analysis = self.detect_background_intelligent(processed_image)\n        \n        # Intelligente Frame-Extraktion\n        frames, extraction_info = self.extract_intelligent_frames(processed_image, foreground_mask)\n        \n        if not frames:\n            return {\n                \"error\": \"No frames extracted\",\n                \"background_analysis\": bg_analysis,\n                \"extraction_info\": extraction_info\n            }\n        \n        # Verarbeite jeden Frame\n        sprite_dir = self.session_dir / \"individual_sprites\" / image_path.stem\n        sprite_dir.mkdir(exist_ok=True)\n        \n        frame_info = []\n        processed_frames = []\n        \n        for i, frame in enumerate(frames, 1):\n            # Vaporwave-Filter anwenden\n            vaporwave_frame = self.apply_enhanced_vaporwave(frame, 0.375)\n            \n            # Als PNG speichern (mit Transparenz)\n            frame_filename = f\"frame_{i:03d}.png\"\n            frame_path = sprite_dir / frame_filename\n            \n            # Konvertiere zu RGBA f√ºr Transparenz\n            frame_rgba = cv2.cvtColor(vaporwave_frame, cv2.COLOR_BGR2BGRA)\n            \n            # Speichere als PNG\n            cv2.imwrite(str(frame_path), frame_rgba)\n            \n            frame_info.append({\n                \"id\": i,\n                \"filename\": frame_filename,\n                \"size\": f\"{frame.shape[1]}x{frame.shape[0]}\",\n                \"area\": frame.shape[0] * frame.shape[1],\n                \"vaporwave_applied\": True\n            })\n            \n            processed_frames.append(vaporwave_frame)\n        \n        # Erstelle GIF Animation\n        gif_path = self.session_dir / \"animations\" / f\"{image_path.stem}_intelligent_vaporwave.gif\"\n        self.create_gif_animation(processed_frames, gif_path)\n        \n        # Speichere Debug-Informationen\n        debug_path = self.session_dir / \"debug_images\" / f\"{image_path.stem}_analysis.png\"\n        self.save_debug_visualization(processed_image, foreground_mask, debug_path)\n        \n        return {\n            \"input_file\": str(image_path),\n            \"processing_timestamp\": datetime.now().isoformat(),\n            \"workflow\": \"Intelligent Analysis + Enhanced\",\n            \"original_size\": original_size,\n            \"upscaled_size\": processed_image.shape[:2],\n            \"background_analysis\": bg_analysis,\n            \"extraction_info\": extraction_info,\n            \"total_frames\": len(frames),\n            \"frames\": frame_info,\n            \"output_directory\": str(sprite_dir),\n            \"gif_animation\": str(gif_path),\n            \"debug_visualization\": str(debug_path)\n        }\n\n    def create_gif_animation(self, frames: List[np.ndarray], output_path: Path):\n        \"\"\"Erstelle GIF Animation aus Frames\"\"\"\n        if not frames:\n            return\n        \n        # Konvertiere zu PIL Images\n        pil_frames = []\n        for frame in frames:\n            rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n            pil_frames.append(Image.fromarray(rgb_frame))\n        \n        # Speichere als GIF\n        if pil_frames:\n            pil_frames[0].save(\n                str(output_path),\n                save_all=True,\n                append_images=pil_frames[1:],\n                duration=200,  # 200ms pro Frame\n                loop=0,\n                optimize=True\n            )\n\n    def save_debug_visualization(self, image: np.ndarray, mask: np.ndarray, output_path: Path):\n        \"\"\"Speichere Debug-Visualisierung\"\"\"\n        h, w = image.shape[:2]\n        \n        # Erstelle Visualisierung: Original + Maske + Overlay\n        debug_img = np.zeros((h, w*3, 3), dtype=np.uint8)\n        \n        # Original\n        debug_img[:, :w] = image\n        \n        # Maske\n        mask_colored = np.zeros((h, w, 3), dtype=np.uint8)\n        mask_colored[mask] = [0, 255, 0]  # Gr√ºn f√ºr Vordergrund\n        debug_img[:, w:2*w] = mask_colored\n        \n        # Overlay\n        overlay = image.copy()\n        overlay[~mask] = overlay[~mask] * 0.3  # Dunkle Hintergrund\n        debug_img[:, 2*w:] = overlay\n        \n        cv2.imwrite(str(output_path), debug_img)\n\n\nif __name__ == \"__main__\":\n    # Test mit dem problematischen Fall\n    analyzer = IntelligentSpritesheetAnalyzer()\n    \n    test_file = Path(\"input/Der_Zauber_des_alten_Mannes.png\")\n    if test_file.exists():\n        print(\"üî¨ INTELLIGENTE ANALYSE - PROBLEMATISCHER FALL\")\n        print(\"=\"*60)\n        \n        result = analyzer.process_single_spritesheet(test_file)\n        \n        # Speichere Report\n        report_path = analyzer.session_dir / \"reports\" / f\"{test_file.stem}_intelligent_report.json\"\n        with open(report_path, 'w', encoding='utf-8') as f:\n            json.dump(result, f, indent=2, ensure_ascii=False)\n        \n        print(f\"\\nüìä ERGEBNIS:\")\n        print(f\"   Frames extrahiert: {result.get('total_frames', 'N/A')}\")\n        print(f\"   Empfohlene Anzahl: {result.get('extraction_info', {}).get('analysis', {}).get('recommendations', {}).get('expected_sprite_count', 'N/A')}\")\n        print(f\"   Report gespeichert: {report_path}\")\n        print(f\"   Debug-Bild: {result.get('debug_visualization', 'N/A')}\")\n        \n    else:\n        print(f\"‚ùå Test-Datei nicht gefunden: {test_file}\")\n
